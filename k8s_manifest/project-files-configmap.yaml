apiVersion: v1
data:
  .env: FINNHUB_TOKEN = d03aeb1r01qvvb9384c0d03aeb1r01qvvb9384cg
  __init__.py: ""
  config.py: "from dotenv import load_dotenv\nimport os \nimport yaml \nload_dotenv('/project/.env')\n\n\n\nclass
    Config :\n    with open('/project/config/config.yml','r') as file:\n        config_data
    = yaml.load(file , Loader=yaml.FullLoader)\n\n        producer_config = config_data[\"PRODUCER\"]['PRODUCER_CONFIG']\n
    \       topic_name = config_data[\"PRODUCER\"]['TOPIC_NAME']\n        server =
    config_data[\"CONSUMER\"]['SERVER']\n\n\n\n\n        finnhub_token = os.getenv('FINNHUB_TOKEN')\n\n\n\n\n\n\n\nconfig
    = Config()    \n#print(config.finnhub_token)"
  config.yml: |-
    PRODUCER:
      PRODUCER_CONFIG: {'bootstrap.servers': 'kafka:9092'}
      TOPIC_NAME: 'finnhub_trades'

    CONSUMER:
      SERVER: 'kafka:9092'
  consumer.py: "from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import
    col, from_json, current_timestamp, avg, count, window\nfrom pyspark.sql.types
    import StructType, StructField, StringType, DoubleType, LongType, ArrayType\nimport
    logging\nfrom cassandra.cluster import Cluster, NoHostAvailable\nimport uuid\nfrom
    config.config import config\n\nlogger = logging.getLogger(__name__)\n\ndef store_cassandra_raw(batch_df,
    batch_id):\n    cluster = None\n    try:\n        logger.info(f'Storing raw batch
    {batch_id} in Cassandra')\n        cluster = Cluster(['cassandra.finstream.svc.cluster.local'],
    port=9042)\n        session = cluster.connect()\n\n        session.execute(\"CREATE
    KEYSPACE IF NOT EXISTS fintech WITH replication = {'class': 'SimpleStrategy',
    'replication_factor': 1}\")\n        session.set_keyspace(\"fintech\")\n        \n
    \       session.execute(\"\"\"CREATE TABLE IF NOT EXISTS fintech.trades (\n            id
    uuid PRIMARY KEY,\n            symbol text,\n            price double,\n            volume
    double,\n            trade_timestamp bigint,\n            ingestion_time timestamp\n
    \       )\"\"\")\n\n        df = batch_df.select(\n            col(\"symbol\").alias(\"symbol\"),\n
    \           col(\"price\").alias(\"price\"),\n            col(\"volume\").alias(\"volume\"),\n
    \           col(\"timestamp\").alias(\"trade_timestamp\"),\n            current_timestamp().alias(\"ingestion_time\")\n
    \       )\n\n        logger.info(f'Starting to insert raw data for batch {batch_id}')\n
    \       rows = df.collect()\n        for row in rows:\n            session.execute(\n
    \               \"\"\"INSERT INTO fintech.trades (id, symbol, price, volume, trade_timestamp,
    ingestion_time)\n                VALUES (%s, %s, %s, %s, %s, %s)\"\"\",\n                (uuid.uuid4(),
    row[\"symbol\"], row[\"price\"], row[\"volume\"], row[\"trade_timestamp\"], row[\"ingestion_time\"])\n
    \           )\n        logger.info(f'Inserted {len(rows)} raw rows into Cassandra
    for batch {batch_id}')\n\n    except NoHostAvailable as e:\n        logger.exception(f'Cassandra
    connection failed for raw batch {batch_id}: {e}')\n    except Exception as e:\n
    \       logger.exception(f'Cassandra raw processing error for batch {batch_id}:
    {e}')\n    finally:\n        if cluster:\n            cluster.shutdown()\n\ndef
    store_cassandra_aggregated(batch_df, batch_id):\n    cluster = None\n    try:\n
    \       logger.info(f'Storing aggregated batch {batch_id} in Cassandra, row count:
    {batch_df.count()}')\n        if batch_df.count() == 0:\n            logger.info(f'No
    data in aggregated batch {batch_id}')\n            return\n\n        cluster =
    Cluster(['cassandra.finstream.svc.cluster.local'], port=9042)\n        session
    = cluster.connect()\n        session.set_keyspace(\"fintech\")\n\n        session.execute(\"\"\"CREATE
    TABLE IF NOT EXISTS fintech.trade_aggregations (\n            window_start timestamp,\n
    \           window_end timestamp,\n            symbol text,\n            avg_price
    double,\n            trade_count bigint,\n            PRIMARY KEY (window_start,
    symbol)\n        )\"\"\")\n\n        df = batch_df.select(\n            col(\"window.start\").alias(\"window_start\"),\n
    \           col(\"window.end\").alias(\"window_end\"),\n            col(\"symbol\").alias(\"symbol\"),\n
    \           col(\"avg_price\").alias(\"avg_price\"),\n            col(\"trade_count\").alias(\"trade_count\")\n
    \       )\n\n        logger.info(f'Starting to insert aggregated data for batch
    {batch_id}')\n        rows = df.collect()\n        for row in rows:\n            session.execute(\n
    \               \"\"\"INSERT INTO fintech.trade_aggregations (window_start, window_end,
    symbol, avg_price, trade_count)\n                VALUES (%s, %s, %s, %s, %s)\"\"\",\n
    \               (row[\"window_start\"], row[\"window_end\"], row[\"symbol\"],
    row[\"avg_price\"], row[\"trade_count\"])\n            )\n        logger.info(f'Inserted
    {len(rows)} aggregated rows into Cassandra for batch {batch_id}')\n\n    except
    NoHostAvailable as e:\n        logger.exception(f'Cassandra connection failed
    for aggregated batch {batch_id}: {e}')\n    except Exception as e:\n        logger.exception(f'Cassandra
    aggregated processing error for batch {batch_id}: {e}')\n    finally:\n        if
    cluster:\n            cluster.shutdown()\n\ndef consumer():\n    spark = None\n
    \   try:\n        logger.info('Starting the consumer')\n        spark = SparkSession.builder
    \\\n            .appName(\"FinStreamProcessing\") \\\n            .config(\"spark.jars.packages\",
    \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,com.datastax.spark:spark-cassandra-connector_2.12:3.5.1,com.github.jnr:jnr-posix:3.1.15\")
    \\\n            .config(\"spark.cassandra.connection.host\", \"cassandra.finstream.svc.cluster.local\")
    \\\n            .config(\"spark.cassandra.connection.port\", \"9042\") \\\n            .getOrCreate()\n
    \       logger.info('Spark session created successfully')\n\n        schema =
    StructType([\n            StructField(\"data\", ArrayType(StructType([\n                StructField(\"c\",
    StringType(), True),\n                StructField(\"p\", DoubleType(), True),\n
    \               StructField(\"s\", StringType(), True),\n                StructField(\"t\",
    LongType(), True),\n                StructField(\"v\", DoubleType(), True)\n            ])),
    True),\n            StructField(\"type\", StringType(), True)\n        ])\n\n
    \       df = spark.readStream \\\n            .format('kafka') \\\n            .option(\"kafka.bootstrap.servers\",
    'kafka:9092') \\\n            .option(\"subscribe\", 'finnhub_trades') \\\n            .option(\"startingOffsets\",
    \"earliest\") \\\n            .option(\"group.id\", \"finstream-group\") \\\n
    \           .load()\n\n        df = df.select(from_json(col(\"value\").cast(\"string\"),
    schema).alias(\"data\")) \\\n               .selectExpr(\"explode(data.data) as
    trade\") \\\n               .select(\n                   col(\"trade.s\").alias(\"symbol\"),\n
    \                  col(\"trade.p\").alias(\"price\"),\n                   col(\"trade.v\").alias(\"volume\"),\n
    \                  col(\"trade.t\").alias(\"timestamp\")\n               )\n\n
    \       raw_query = df.writeStream \\\n            .foreachBatch(store_cassandra_raw)
    \\\n            .outputMode('append') \\\n            .start()\n        logger.info('Raw
    data streaming query started')\n\n        aggregated_df = df.groupBy(\n            window((col(\"timestamp\")
    / 1000).cast(\"timestamp\"), \"5 seconds\"),\n            col(\"symbol\")\n        ).agg(\n
    \           avg(\"price\").alias(\"avg_price\"),\n            count(\"*\").alias(\"trade_count\")\n
    \       )\n\n        agg_query = aggregated_df.writeStream \\\n            .foreachBatch(store_cassandra_aggregated)
    \\\n            .outputMode('update') \\\n            .start()\n        logger.info('Aggregated
    data streaming query started')\n\n        spark.streams.awaitAnyTermination()\n\n
    \   except Exception as e:\n        logger.exception(f'Consumer error: {e}')\n
    \   finally:\n        if spark:\n            spark.stop()\n\nif __name__ == \"__main__\":\n
    \   logging.basicConfig(level=logging.INFO)\n    consumer()"
  producer.py: |-
    from confluent_kafka import Producer
    from confluent_kafka.admin import AdminClient, NewTopic
    import json
    import logging
    import websocket
    from config.config import config

    logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', filename="/project/log/app.log", filemode='a')
    logger = logging.getLogger(__name__)

    def report(err, msg):
        if err is not None:
            logger.error(f'Message failed from the producer: {err}')
        else:
            logger.info(f'Message delivered to: topic = {msg.topic()} & partition = {msg.partition()}')

    def on_message(ws, message):
        try:
            logger.info(f'Received WebSocket message: {message}')
            producer.produce(config.topic_name, value=message, callback=report)
            producer.flush()
        except Exception as e:
            logger.error(f'Error producing message to Kafka: {e}')

    def on_error(ws, error):
        logger.error(f'WebSocket error: {error}')

    def on_close(ws, close_status_code, close_msg):
        logger.info(f'WebSocket closed: {close_status_code}, {close_msg}')

    def on_open(ws):
        logger.info('WebSocket connection opened')
        ws.send('{"type":"subscribe","symbol":"AAPL"}')
        ws.send('{"type":"subscribe","symbol":"AMZN"}')
        ws.send('{"type":"subscribe","symbol":"BINANCE:BTCUSDT"}')
        ws.send('{"type":"subscribe","symbol":"IC MARKETS:1"}')

    def producer():
        global producer
        try:
            admin = AdminClient(config.producer_config)
            if config.topic_name not in admin.list_topics().topics:
                topic = NewTopic(config.topic_name, num_partitions=1, replication_factor=1)
                admin.create_topics([topic])
                logger.info(f'Topic created: {config.topic_name}')
            else:
                logger.info(f'Topic already exists: {config.topic_name}')

            # initialize Kafka producer
            producer = Producer(config.producer_config)

            # start WebSocket connection
            websocket.enableTrace(False)
            ws = websocket.WebSocketApp(
                f"wss://ws.finnhub.io?token={config.finnhub_token}",
                on_message=on_message,
                on_error=on_error,
                on_close=on_close
            )
            ws.on_open = on_open
            ws.run_forever()  # Runs indefinitely, keeping container alive

        except Exception as e:
            logger.exception('Producer error')

    producer()
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: project-files
  namespace: finstream
